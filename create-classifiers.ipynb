{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12740577,"sourceType":"datasetVersion","datasetId":8053565},{"sourceId":13054789,"sourceType":"datasetVersion","datasetId":8266842},{"sourceId":13065521,"sourceType":"datasetVersion","datasetId":8274223}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Load the DataFrame from Parquet\nimport pandas as pd\n\ndf_all = pd.read_parquet(\"/kaggle/input/parquet-df-audit-opinions\")\nprint(\"Loaded DataFrame with shape:\", df_all.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:12.057853Z","iopub.execute_input":"2025-09-28T06:09:12.058110Z","iopub.status.idle":"2025-09-28T06:09:17.412677Z","shell.execute_reply.started":"2025-09-28T06:09:12.058087Z","shell.execute_reply":"2025-09-28T06:09:17.411730Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set max rows to 3000 so full output shows\npd.set_option(\"display.max_rows\", 3000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:17.414070Z","iopub.execute_input":"2025-09-28T06:09:17.415579Z","iopub.status.idle":"2025-09-28T06:09:17.419941Z","shell.execute_reply.started":"2025-09-28T06:09:17.415550Z","shell.execute_reply":"2025-09-28T06:09:17.419046Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, classification_report,  confusion_matrix\n\nfrom sklearn.exceptions import UndefinedMetricWarning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nimport warnings\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:17.420850Z","iopub.execute_input":"2025-09-28T06:09:17.421126Z","iopub.status.idle":"2025-09-28T06:09:18.283043Z","shell.execute_reply.started":"2025-09-28T06:09:17.421101Z","shell.execute_reply":"2025-09-28T06:09:18.281900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your combined dataframe (df_all) here, with columns: ['year', 'text', 'label', 'source', 'llm']\n\n# Filter only relevant years for training/testing\ndf_all = df_all[(df_all['year'] >= 1998) & (df_all['year'] <= 2021)].reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:18.284127Z","iopub.execute_input":"2025-09-28T06:09:18.284609Z","iopub.status.idle":"2025-09-28T06:09:18.313190Z","shell.execute_reply.started":"2025-09-28T06:09:18.284579Z","shell.execute_reply":"2025-09-28T06:09:18.312274Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_all","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:18.315335Z","iopub.execute_input":"2025-09-28T06:09:18.315640Z","iopub.status.idle":"2025-09-28T06:09:18.349588Z","shell.execute_reply.started":"2025-09-28T06:09:18.315610Z","shell.execute_reply":"2025-09-28T06:09:18.348748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_all[df_all['source']=='original']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:18.350544Z","iopub.execute_input":"2025-09-28T06:09:18.350842Z","iopub.status.idle":"2025-09-28T06:09:18.387785Z","shell.execute_reply.started":"2025-09-28T06:09:18.350794Z","shell.execute_reply":"2025-09-28T06:09:18.386954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_all.describe()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:18.388751Z","iopub.execute_input":"2025-09-28T06:09:18.389085Z","iopub.status.idle":"2025-09-28T06:09:18.422318Z","shell.execute_reply.started":"2025-09-28T06:09:18.389057Z","shell.execute_reply":"2025-09-28T06:09:18.421407Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## STACKED MODEL 1 (BEST OF ALL WITH REGARD TO F1 SCORE)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n\n# ---------------------------\n# Helper: compute F-beta\n# ---------------------------\ndef fbeta_score_custom(y_true, y_pred, beta=2):\n    prec, rec, _, _ = precision_recall_fscore_support(\n        y_true, y_pred, beta=beta, average=\"binary\", zero_division=0\n    )\n    if prec + rec == 0:\n        return 0\n    return (1 + beta**2) * (prec * rec) / ((beta**2 * prec) + rec)\n\n# ---------------------------\n# Validation curve table\n# ---------------------------\ndef validation_curve_table(y_true, scores, beta=2):\n    sorted_idx = np.argsort(scores)[::-1]\n    y_true_sorted = y_true[sorted_idx]\n    scores_sorted = scores[sorted_idx]\n\n    best_thr, best_fbeta = None, -1\n    tp, fp, fn = 0, 0, int(np.sum(y_true_sorted))\n\n    for i in range(len(scores_sorted)):\n        if y_true_sorted[i] == 1:\n            tp += 1\n            fn -= 1\n        else:\n            fp += 1\n        prec = tp / (tp + fp) if (tp + fp) > 0 else 0\n        rec = tp / (tp + fn) if (tp + fn) > 0 else 0\n        fbeta = (1 + beta**2) * prec * rec / ((beta**2 * prec) + rec) if (prec + rec) > 0 else 0\n        if fbeta > best_fbeta:\n            best_fbeta = fbeta\n            best_thr = scores_sorted[i]\n    return best_thr, best_fbeta\n\n# ---------------------------\n# Base model trainer with class weights\n# ---------------------------\ndef train_base_model(X_train, y_train, model_type='sgd', param_grid=None, beta=2):\n    best_model, best_params, best_threshold, best_fbeta = None, None, 0.5, -1\n\n    for params in ParameterGrid(param_grid):\n        if model_type == 'sgd':\n            clf = SGDClassifier(random_state=42, class_weight='balanced', **params)\n        elif model_type == 'linsvc':\n            clf = LinearSVC(random_state=42, class_weight='balanced', **params)\n        elif model_type == 'rbf':\n            clf = SVC(kernel='rbf', probability=False, random_state=42, class_weight='balanced', **params)\n        elif model_type == 'nb':\n            clf = MultinomialNB(**params)\n        else:\n            raise ValueError(\"Unknown model_type\")\n\n        clf.fit(X_train, y_train)\n\n        if model_type == 'nb':\n            y_scores = clf.predict_proba(X_train)[:, 1]\n        else:\n            y_scores = clf.decision_function(X_train)\n        thr, fbeta = validation_curve_table(np.array(y_train), y_scores, beta=beta)\n\n        if fbeta > best_fbeta:\n            best_fbeta, best_model, best_params, best_threshold = fbeta, clf, params, thr\n\n    return best_model, best_params, best_threshold, best_fbeta\n\n# ---------------------------\n# Stacked evaluation per year\n# ---------------------------\ndef evaluate_year_stacked(df, test_year, beta=2):\n    train_years = list(range(test_year - 4, test_year))\n    df_trainval = df[df['year'].isin(train_years)].copy()\n\n    # Keep the original validation split using train_test_split\n    df_train, df_val = train_test_split(\n        df_trainval, test_size=0.25, stratify=df_trainval['label'], random_state=42\n    )\n\n    df_test = df[(df['year'] == test_year) & (df['source'] == 'original')].copy()\n\n    df_train_orig = df_train[df_train['source'] == 'original']\n    df_train_syn = df_train[df_train['source'] != 'original']\n\n    # Initialize vectorizer with more features for more aggressive approach\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=30000)  # Increased from 10000\n    vectorizer.fit(pd.concat([df_train_orig['text'], df_train_syn['text']]))\n\n    # Transform all data\n    X_train_vec = vectorizer.transform(pd.concat([df_train_orig['text'], df_train_syn['text']]))\n    y_train_vec = pd.concat([df_train_orig['label'], df_train_syn['label']])\n    X_val_vec = vectorizer.transform(df_val['text'])\n    y_val_vec = df_val['label'].values\n    X_test_vec = vectorizer.transform(df_test['text'])\n    y_test_vec = df_test['label'].values\n\n    # Base model grids with less regularization for more aggressive approach\n    param_grid_sgd = {'loss': ['hinge', 'log_loss'], 'alpha': [1e-4, 1e-3], 'penalty': ['l2', 'l1']}\n    param_grid_linsvc = {'C': [0.1, 0.5, 1.0]}  # Higher C values for less regularization\n    param_grid_rbf = {'C': [0.5, 1.0], 'gamma': ['scale']}  # Added back RBF SVM\n    param_grid_nb = {'alpha': [0.5, 1.0, 2.0]}\n\n    # Train base models\n    base_models = {}\n    for name, grid in [('sgd', param_grid_sgd), ('linsvc', param_grid_linsvc), ('rbf', param_grid_rbf), ('nb', param_grid_nb)]:\n        print(f\"[{test_year}] Processing base model: {name}\")\n        model, best_params, best_thr, best_fbeta = train_base_model(\n            X_train_vec, y_train_vec, model_type=name, param_grid=grid, beta=beta\n        )\n        base_models[name] = (model, best_thr)\n        print(f\"[{test_year}] -> base {name} chosen setting: {best_params}, OOF-F{beta}(approx)={best_fbeta:.3f}\")\n\n    # Meta features\n    def get_decision_matrix(models, X):\n        meta_features = []\n        for name, (model, thr) in models.items():\n            if name == 'nb':\n                scores = model.predict_proba(X)[:, 1]\n            else:\n                scores = model.decision_function(X)\n            meta_features.append(scores.reshape(-1, 1))\n        return np.hstack(meta_features)\n\n    X_meta_val = get_decision_matrix(base_models, X_val_vec)\n    X_meta_test = get_decision_matrix(base_models, X_test_vec)\n\n    # Train meta model with less regularization\n    meta_clf = LogisticRegression(\n        random_state=42, \n        max_iter=200, \n        class_weight='balanced',\n        C=0.5,  # Less regularization\n        penalty='l2'\n    )\n    meta_clf.fit(X_meta_val, y_val_vec)\n\n    # Tune threshold on validation with F2 for balanced precision/recall\n    val_scores = meta_clf.predict_proba(X_meta_val)[:, 1]\n    thresholds = np.linspace(0.2, 0.8, 61)  # Wider threshold range for more aggressive approach\n    best_fbeta, best_thr = -1, 0.5\n    for thr in thresholds:\n        y_pred_val = (val_scores >= thr).astype(int)\n        fbeta_val = fbeta_score_custom(y_val_vec, y_pred_val, beta=2)  # F2 for balanced approach\n        if fbeta_val > best_fbeta:\n            best_fbeta, best_thr = fbeta_val, thr\n    print(f\"[{test_year}] Meta OOF best threshold={best_thr:.4f}, OOF-F2={best_fbeta:.3f}\")\n\n    # Predict test\n    test_scores = meta_clf.predict_proba(X_meta_test)[:, 1]\n    y_pred_test = (test_scores >= best_thr).astype(int)\n\n    tn, fp, fn, tp = confusion_matrix(y_test_vec, y_pred_test, labels=[0, 1]).ravel()\n    f1 = f1_score(y_test_vec, y_pred_test)\n\n    print(f\"\\n=== Year {test_year} Results ===\")\n    print(f\"TP={tp}, FP={fp}, FN={fn}, F1={f1:.3f}\")\n    print(f\"[{test_year}] Stored {len(df_test)} test rows, predicted 1s: {int(np.sum(y_pred_test))}\\n\")\n\n    df_test_result = df_test[['year', 'text', 'firm_id']].copy()  # Include firm_id\n    df_test_result['true_label'] = y_test_vec\n    df_test_result['pred_label'] = y_pred_test\n\n    return df_test_result\n\n# ---------------------------\n# Iterate all years with cumulative metrics\n# ---------------------------\nall_years_results = []\ndf_all_predicted = []\ncumulative_metrics = {'TP': 0, 'FP': 0, 'FN': 0}\n\nfor test_year in range(2000, 2022):\n    print(f\"\\n########### START YEAR {test_year} ###########\")\n    result = evaluate_year_stacked(df_all, test_year, beta=2)\n\n    df_all_predicted.append(result)\n\n    tn, fp, fn, tp = confusion_matrix(result['true_label'], result['pred_label'], labels=[0, 1]).ravel()\n    f1 = f1_score(result['true_label'], result['pred_label'])\n    all_years_results.append({'year': test_year, 'TP': tp, 'FP': fp, 'FN': fn, 'F1': f1})\n    \n    # Update cumulative metrics\n    cumulative_metrics['TP'] += tp\n    cumulative_metrics['FP'] += fp\n    cumulative_metrics['FN'] += fn\n    \n    # Calculate cumulative F1\n    cumulative_F1 = (2 * cumulative_metrics['TP']) / (\n        2 * cumulative_metrics['TP'] + cumulative_metrics['FP'] + cumulative_metrics['FN']\n    ) if (2 * cumulative_metrics['TP'] + cumulative_metrics['FP'] + cumulative_metrics['FN']) > 0 else 0\n\n    print(f\"=== End YEAR {test_year} ===\")\n    print(f\"Year {test_year}: TP={tp}, FP={fp}, FN={fn}, F1={f1:.3f}\")\n    print(f\"Cumulative (2000-{test_year}): TP={cumulative_metrics['TP']}, FP={cumulative_metrics['FP']}, FN={cumulative_metrics['FN']}, F1={cumulative_F1:.3f}\")\n    print(f\"########### END YEAR {test_year} ###########\\n\")\n\n# Combine predictions\ndf_all_predicted = pd.concat(df_all_predicted, ignore_index=True)\n\n# Overall metrics\noverall_TP = sum(r['TP'] for r in all_years_results)\noverall_FP = sum(r['FP'] for r in all_years_results)\noverall_FN = sum(r['FN'] for r in all_years_results)\noverall_F1 = (2 * overall_TP) / (2 * overall_TP + overall_FP + overall_FN)\n\nprint(\"\\n=== Overall Summary across all years ===\")\nprint(f\"Overall TP={overall_TP}, FP={overall_FP}, FN={overall_FN}, F1={overall_F1:.3f}\")\n\n# Store predicted 1s with firm_id\ndf_predicted_1s = df_all_predicted[df_all_predicted['pred_label'] == 1][['year', 'firm_id', 'text', 'true_label']]\nprint(f\"\\nTotal predicted 1s across all years: {len(df_predicted_1s)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T20:22:12.860109Z","iopub.execute_input":"2025-09-13T20:22:12.860531Z","iopub.status.idle":"2025-09-13T22:39:39.745308Z","shell.execute_reply.started":"2025-09-13T20:22:12.860506Z","shell.execute_reply":"2025-09-13T22:39:39.743919Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# --- Save ---\nwith open(\"df_predicted_1s.pkl\", \"wb\") as f:\n    pickle.dump(df_predicted_1s, f)\n\nprint(\"✅ df_predicted_1s saved to df_predicted_1s.pkl\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T22:39:39.747698Z","iopub.execute_input":"2025-09-13T22:39:39.748008Z","iopub.status.idle":"2025-09-13T22:39:39.760871Z","shell.execute_reply.started":"2025-09-13T22:39:39.747986Z","shell.execute_reply":"2025-09-13T22:39:39.759743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save as CSV\ndf_predicted_1s.to_csv(\"df_predicted_1s.csv\", index=False, encoding='utf-8')\nprint(\"✅ df_predicted_1s saved to df_predicted_1s.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-13T22:39:39.761767Z","iopub.execute_input":"2025-09-13T22:39:39.762047Z","iopub.status.idle":"2025-09-13T22:39:39.876199Z","shell.execute_reply.started":"2025-09-13T22:39:39.762018Z","shell.execute_reply":"2025-09-13T22:39:39.875342Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## STACKED MODEL 2 (2ND TO BEST)","metadata":{}},{"cell_type":"code","source":"# =====================================================\n# Document Classification Pipeline (Yearly Rolling)\n# =====================================================\n# Requirements:\n#   pip install scikit-learn pandas numpy tqdm\n#\n# Expected DataFrame: df_all with columns:\n#   - 'year' (int)\n#   - 'text' (str)\n#   - 'label' (0/1)\n#   - 'source' ('original' or 'synthetic')\n# =====================================================\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, fbeta_score\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\n\nSEED = 42\nnp.random.seed(SEED)\n\n# =====================================================\n# Helper Functions\n# =====================================================\n\ndef choose_threshold_by_f2(y_true, scores, beta=2):\n    \"\"\"\n    Scan thresholds, choose the one maximizing F-beta on validation.\n    \"\"\"\n    thresholds = np.linspace(0.05, 0.95, 50)\n    best_thr, best_fbeta = 0.5, -1\n    for thr in thresholds:\n        preds = (scores >= thr).astype(int)\n        fbeta = fbeta_score(y_true, preds, beta=beta, zero_division=0)\n        if fbeta > best_fbeta:\n            best_fbeta, best_thr = fbeta, thr\n    return best_thr, best_fbeta\n\n\ndef evaluate_year(df, test_year, beta=2, ngram_range=(1,2), max_features=20000):\n    \"\"\"\n    Train on years (t-5 ... t-2), validate on year (t-1), test on year (t).\n    \"\"\"\n    train_years = list(range(test_year - 5, test_year - 1))\n    val_year = test_year - 1\n\n    df_train = df[df[\"year\"].isin(train_years) ]\n    df_val = df[(df[\"year\"] == val_year) ]\n    df_test = df[(df[\"year\"] == test_year) & (df[\"source\"] == \"original\")]\n\n    if df_train.empty or df_val.empty or df_test.empty:\n        print(f\"[{test_year}] Skipped due to insufficient data.\")\n        return None\n\n    # TF-IDF vectorizer fit on train+val (NOT test → avoids lookahead bias)\n    vectorizer = TfidfVectorizer(ngram_range=ngram_range, max_features=max_features)\n    vectorizer.fit(pd.concat([df_train[\"text\"], df_val[\"text\"]]))\n\n    X_train = vectorizer.transform(df_train[\"text\"])\n    y_train = df_train[\"label\"].values\n    X_val = vectorizer.transform(df_val[\"text\"])\n    y_val = df_val[\"label\"].values\n    X_test = vectorizer.transform(df_test[\"text\"])\n    y_test = df_test[\"label\"].values\n\n    # Logistic Regression (balanced)\n    clf = LogisticRegression(\n        solver=\"saga\",\n        max_iter=200,\n        class_weight=\"balanced\",\n        C=1.0,\n        random_state=SEED,\n    )\n    clf.fit(X_train, y_train)\n\n    # Validation threshold tuning\n    val_scores = clf.predict_proba(X_val)[:, 1]\n    thr, best_f2 = choose_threshold_by_f2(y_val, val_scores, beta=beta)\n\n    # Test predictions\n    test_scores = clf.predict_proba(X_test)[:, 1]\n    y_pred = (test_scores >= thr).astype(int)\n\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, labels=[0, 1]).ravel()\n    f2 = fbeta_score(y_test, y_pred, beta=beta, zero_division=0)\n\n    print(f\"\\n=== Year {test_year} Results ===\")\n    print(f\"Threshold={thr:.3f}, Val-F{beta}={best_f2:.3f}, Test-F{beta}={f2:.3f}\")\n    print(f\"TP={tp}, FP={fp}, FN={fn}, Total Test={len(df_test)}\")\n\n    df_test_result = df_test[[\"year\", \"text\",\"firm_id\"]].copy()\n    df_test_result[\"true_label\"] = y_test\n    df_test_result[\"pred_label\"] = y_pred\n    df_test_result[\"score\"] = test_scores\n\n    return df_test_result, {\"year\": test_year, \"TP\": tp, \"FP\": fp, \"FN\": fn, f\"F{beta}\": f2}\n\n\n# =====================================================\n# Execution Loop\n# =====================================================\n\nall_results = []\ndf_all_preds = []\n\nfor year in tqdm(range(2000, 2022), desc=\"Processing years\"):\n    res = evaluate_year(df_all, year, beta=2)\n    if res is not None:\n        df_preds, metrics = res\n        df_all_preds.append(df_preds)\n        all_results.append(metrics)\n\n        # --- Cumulative metrics up to this year ---\n        df_metrics = pd.DataFrame(all_results)\n        cum_TP = df_metrics[\"TP\"].sum()\n        cum_FP = df_metrics[\"FP\"].sum()\n        cum_FN = df_metrics[\"FN\"].sum()\n        cum_F1 = (2 * cum_TP) / (2 * cum_TP + cum_FN + cum_FP) if (cum_TP + cum_FP + cum_FN) > 0 else 0\n\n        print(f\"\\n--- Cumulative up to {year} ---\")\n        print(f\"Total TP={cum_TP}, FP={cum_FP}, FN={cum_FN}, Cum-F1={cum_F1:.3f}\")\n\n\n# =====================================================\n# Final Summary\n# =====================================================\n\nif df_all_preds:\n    df_all_preds = pd.concat(df_all_preds, ignore_index=True)\nelse:\n    df_all_preds = pd.DataFrame(columns=[\"year\", \"text\", \"true_label\", \"pred_label\", \"score\"])\n\ndf_metrics = pd.DataFrame(all_results)\n\noverall_TP = df_metrics[\"TP\"].sum()\noverall_FP = df_metrics[\"FP\"].sum()\noverall_FN = df_metrics[\"FN\"].sum()\noverall_F2 = (5 * overall_TP) / (5 * overall_TP + 4 * overall_FN + overall_FP) if (overall_TP + overall_FP + overall_FN) > 0 else 0\n\ndf_predicted_1s=df_all_preds[df_all_preds['pred_label']==1]\n\nprint(\"\\n=== Overall Summary ===\")\nprint(f\"Overall TP={overall_TP}, FP={overall_FP}, FN={overall_FN}, F2={overall_F2:.3f}\")\n\n# Save outputs if needed\n# df_all_preds.to_pickle(\"df_all_preds.pkl\")\n# df_metrics.to_csv(\"per_year_metrics.csv\", index=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:46:15.161239Z","iopub.execute_input":"2025-09-14T08:46:15.161542Z","iopub.status.idle":"2025-09-14T08:57:07.440635Z","shell.execute_reply.started":"2025-09-14T08:46:15.161519Z","shell.execute_reply":"2025-09-14T08:57:07.439677Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Save results ---\nimport pickle\n\n# Save as pickle\nwith open(\"df_predicted_1s_2nd_best.pkl\", \"wb\") as f:\n    pickle.dump(df_predicted_1s, f)\nprint(\"✅ df_predicted_1s_2nd_best saved to df_predicted_1s_2nd_best.pkl\")\n\n# Save as CSV\ndf_predicted_1s.to_csv(\"df_predicted_1s_2nd_best.csv.csv\", index=False, encoding='utf-8')\nprint(\"✅ df_predicted_1s_2nd_best saved to df_predicted_1s_2nd_best.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T08:57:07.442139Z","iopub.execute_input":"2025-09-14T08:57:07.442438Z","iopub.status.idle":"2025-09-14T08:57:07.586814Z","shell.execute_reply.started":"2025-09-14T08:57:07.442419Z","shell.execute_reply":"2025-09-14T08:57:07.585879Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## STACKED MODEL 3 (3rd to BEST)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import fbeta_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\n\nRANDOM_STATE = 42\n\n# ------------------------------\n# TF-IDF Vectorizer (train-only fitting)\n# ------------------------------\ndef get_vectorizer(max_features=60000, ngram_range=(1,3)):\n    return TfidfVectorizer(\n        max_features=max_features,\n        ngram_range=ngram_range,\n        analyzer=\"word\",\n        lowercase=True,\n        stop_words=\"english\"\n    )\n\n# ------------------------------\n# Base models (aggressive, recall-focused)\n# ------------------------------\ndef get_base_models():\n    return {\n        \"logreg\": LogisticRegression(\n            penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.7,\n            C=0.5, class_weight=\"balanced\", max_iter=3000, random_state=RANDOM_STATE\n        ),\n        \"linsvc\": CalibratedClassifierCV(\n            LinearSVC(C=0.5, class_weight=\"balanced\", random_state=RANDOM_STATE),\n            cv=3\n        ),\n        \"sgd\": SGDClassifier(\n            loss=\"log_loss\", alpha=1e-6, penalty=\"l1\",\n            class_weight=\"balanced\", random_state=RANDOM_STATE\n        )\n    }\n\n# ------------------------------\n# Threshold tuning (validation only)\n# ------------------------------\ndef pick_threshold(y_val, val_scores, beta=2.0):\n    thresholds = np.linspace(0.01, 0.99, 99)\n    best_thr, best_fbeta = 0.5, -1\n    for thr in thresholds:\n        preds = (val_scores >= thr).astype(int)\n        fbeta = fbeta_score(y_val, preds, beta=beta, zero_division=0)\n        if fbeta > best_fbeta:\n            best_fbeta, best_thr = fbeta, thr\n    return best_thr, best_fbeta\n\n# ------------------------------\n# Evaluate a single year\n# ------------------------------\ndef evaluate_year(df, test_year):\n    df_trainval = df[df[\"year\"] < test_year]\n    df_test = df[(df[\"year\"] == test_year) & (df[\"source\"] == \"original\")]\n\n    if df_trainval.empty or df_test.empty:\n        return None, None\n\n    # Train/val split\n    df_train, df_val = train_test_split(\n        df_trainval, test_size=0.2,\n        stratify=df_trainval[\"label\"], random_state=RANDOM_STATE\n    )\n\n    # Vectorize (fit on train only)\n    vec = get_vectorizer()\n    X_train = vec.fit_transform(df_train[\"text\"])\n    X_val = vec.transform(df_val[\"text\"])\n    X_test = vec.transform(df_test[\"text\"])\n\n    y_train, y_val, y_test = df_train[\"label\"].values, df_val[\"label\"].values, df_test[\"label\"].values\n\n    # Base models\n    models = get_base_models()\n    val_preds, test_preds = [], []\n\n    for name, model in models.items():\n        model.fit(X_train, y_train)\n        val_probs = model.predict_proba(X_val)[:, 1]\n        test_probs = model.predict_proba(X_test)[:, 1]\n        val_preds.append(val_probs)\n        test_preds.append(test_probs)\n\n    # Stacking meta-learner\n    X_val_stack = np.vstack(val_preds).T\n    X_test_stack = np.vstack(test_preds).T\n    meta = LogisticRegression(solver=\"liblinear\", class_weight=\"balanced\", random_state=RANDOM_STATE)\n    meta.fit(X_val_stack, y_val)\n    val_scores = meta.predict_proba(X_val_stack)[:, 1]\n    test_scores = meta.predict_proba(X_test_stack)[:, 1]\n\n    # Threshold selection (validation only)\n    used_thr, best_f2 = pick_threshold(y_val, val_scores, beta=2.0)\n\n    # Apply threshold to test set\n    y_pred_test = (test_scores >= used_thr).astype(int)\n\n    # Metrics\n    TP = ((y_pred_test == 1) & (y_test == 1)).sum()\n    FP = ((y_pred_test == 1) & (y_test == 0)).sum()\n    FN = ((y_pred_test == 0) & (y_test == 1)).sum()\n\n    prec = precision_score(y_test, y_pred_test, zero_division=0)\n    rec = recall_score(y_test, y_pred_test, zero_division=0)\n    f1 = fbeta_score(y_test, y_pred_test, beta=1.0, zero_division=0)\n\n    print(f\"=== {test_year} Results ===\")\n    print(f\"TP={TP}, FP={FP}, FN={FN}, Prec={prec:.3f}, Rec={rec:.3f}, F1={f1:.3f}, thr={used_thr:.3f}\")\n\n    metrics = {\n        \"year\": test_year,\n        \"TP\": TP, \"FP\": FP, \"FN\": FN,\n        \"Precision\": prec, \"Recall\": rec, \"F1\": f1, \"thr\": used_thr\n    }\n    return df_test.assign(pred_score=test_scores, pred_label=y_pred_test, true_label=y_test, used_threshold=used_thr), metrics\n\n# ------------------------------\n# Run multiple years with cumulative metrics\n# ------------------------------\ndef run_all_years(df, start_year, end_year):\n    preds_all, metrics_all = [], []\n    cumulative = {\"TP\": 0, \"FP\": 0, \"FN\": 0}\n\n    for year in range(start_year, end_year + 1):\n        print(f\"\\n#################### START YEAR {year} ####################\")\n        df_out, metrics = evaluate_year(df, year)\n        if df_out is not None:\n            preds_all.append(df_out)\n            metrics_all.append(metrics)\n\n            # Update cumulative metrics\n            cumulative[\"TP\"] += metrics[\"TP\"]\n            cumulative[\"FP\"] += metrics[\"FP\"]\n            cumulative[\"FN\"] += metrics[\"FN\"]\n\n            cum_f1 = (2 * cumulative[\"TP\"]) / (\n                2 * cumulative[\"TP\"] + cumulative[\"FP\"] + cumulative[\"FN\"]\n            ) if (2 * cumulative[\"TP\"] + cumulative[\"FP\"] + cumulative[\"FN\"]) > 0 else 0\n\n            print(f\"=== End YEAR {year} === Cumulative F1 ({start_year}-{year}) = {cum_f1:.3f}\")\n            print(f\"#################### END YEAR {year} ####################\")\n\n    # Overall summary\n    overall_TP = sum(m[\"TP\"] for m in metrics_all)\n    overall_FP = sum(m[\"FP\"] for m in metrics_all)\n    overall_FN = sum(m[\"FN\"] for m in metrics_all)\n    overall_F1 = (2 * overall_TP) / (2 * overall_TP + overall_FP + overall_FN) if (2 * overall_TP + overall_FP + overall_FN) > 0 else 0\n\n    print(f\"\\n=== Overall Summary across all years ===\")\n    print(f\"Overall TP={overall_TP}, FP={overall_FP}, FN={overall_FN}, F1={overall_F1:.3f}\")\n\n    return pd.concat(preds_all, ignore_index=True), pd.DataFrame(metrics_all)\n\n# ------------------------------\n# Main\n# ------------------------------\nif __name__ == \"__main__\":\n    # Expect df_all present in environment with columns: year, firm_id, text, label, source\n    try:\n        df_all\n    except NameError:\n        raise RuntimeError(\"Load df_all first (pandas DataFrame) with columns: year, firm_id, text, label, source\")\n\n    # Run pipeline\n    preds_df, yearly_metrics = run_all_years(df_all, start_year=2000, end_year=2021)\n\n    # Extract predicted 1s and save\n    df_predicted_1s = preds_df[preds_df['pred_label'] == 1][['year','firm_id','text','true_label','pred_score','used_threshold']]\n    print(f\"\\nTotal predicted 1s across all years: {len(df_predicted_1s)}\")\n\n    # Save outputs\n    df_predicted_1s.to_csv(\"df_predicted_1s.csv\", index=False, encoding=\"utf-8\")\n    print(\"✅ Predictions saved to df_predicted_1s.csv\")\n    yearly_metrics.to_csv(\"metrics_summary.csv\", index=False, encoding=\"utf-8\")\n    print(\"✅ Metrics saved to metrics_summary.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-27T14:25:18.346646Z","iopub.execute_input":"2025-09-27T14:25:18.346986Z","iopub.status.idle":"2025-09-27T14:42:24.931131Z","shell.execute_reply.started":"2025-09-27T14:25:18.346965Z","shell.execute_reply":"2025-09-27T14:42:24.930313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# --- Save ---\nwith open(\"df_predicted_1s_3rd_best.pkl\", \"wb\") as f:\n    pickle.dump(df_predicted_1s, f)\n\nprint(\"✅ df_predicted_1s_3rd_best saved to df_predicted_1s_3rd_best.pkl\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:24:06.552985Z","iopub.execute_input":"2025-09-28T06:24:06.553301Z","iopub.status.idle":"2025-09-28T06:24:06.569023Z","shell.execute_reply.started":"2025-09-28T06:24:06.553283Z","shell.execute_reply":"2025-09-28T06:24:06.567952Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save as CSV\ndf_predicted_1s.to_csv(\"df_predicted_1s_3rd_best.csv\", index=False, encoding='utf-8')\nprint(\"✅ df_predicted_1s_3rd_best saved to df_predicted_1s_3rd_best.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:24:07.624673Z","iopub.execute_input":"2025-09-28T06:24:07.624994Z","iopub.status.idle":"2025-09-28T06:24:07.916629Z","shell.execute_reply.started":"2025-09-28T06:24:07.624970Z","shell.execute_reply":"2025-09-28T06:24:07.915716Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## STACKED MODEL 4 (4th to BEST)","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_recall_fscore_support\n\nRANDOM_STATE = 42\n\n# ---------------------------\n# Helpers\n# ---------------------------\ndef fbeta_score_custom(y_true, y_pred, beta=2):\n    prec, rec, _, _ = precision_recall_fscore_support(\n        y_true, y_pred, beta=beta, average=\"binary\", zero_division=0\n    )\n    if prec + rec == 0:\n        return 0.0\n    return (1 + beta**2) * (prec * rec) / ((beta**2 * prec) + rec)\n\ndef pick_threshold(y_val, val_scores, beta=2.0):\n    \"\"\"Return threshold maximizing F-beta on validation.\"\"\"\n    thresholds = np.linspace(0.01, 0.99, 99)\n    best_thr, best_fbeta = 0.5, -1\n    for thr in thresholds:\n        preds = (val_scores >= thr).astype(int)\n        fbeta = fbeta_score_custom(y_val, preds, beta=beta)\n        if fbeta > best_fbeta:\n            best_fbeta, best_thr = fbeta, thr\n    return best_thr, best_fbeta\n\n# ---------------------------\n# Base models for Version B2\n# ---------------------------\ndef get_base_models_B2():\n    return {\n        \"logreg\": LogisticRegression(\n            penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.7,\n            C=0.5, class_weight=\"balanced\", max_iter=3000, random_state=RANDOM_STATE\n        ),\n        \"linsvc\": CalibratedClassifierCV(\n            LinearSVC(C=0.5, class_weight=\"balanced\", random_state=RANDOM_STATE),\n            cv=3\n        ),\n        \"sgd\": SGDClassifier(\n            loss=\"log_loss\", alpha=1e-6, penalty=\"l1\",\n            class_weight=\"balanced\", random_state=RANDOM_STATE\n        )\n    }\n\n# ---------------------------\n# Evaluate single year (Version B2)\n# ---------------------------\ndef evaluate_year_version_b2(df, test_year, beta=2):\n    train_years = list(range(test_year - 4, test_year))\n    df_trainval = df[df['year'].isin(train_years)].copy()\n    if df_trainval.empty:\n        print(f\"[{test_year}] Not enough training data (train_years={train_years}) -> skip\")\n        return None\n\n    # stratified train/val split\n    df_train, df_val = train_test_split(\n        df_trainval, test_size=0.25, stratify=df_trainval['label'], random_state=RANDOM_STATE\n    )\n\n    # test set (original only)\n    df_test = df[(df['year'] == test_year) & (df['source'] == 'original')].copy()\n    if df_test.empty:\n        print(f\"[{test_year}] No test rows (original) -> skip\")\n        return None\n\n    # TF-IDF fit on training only\n    vectorizer = TfidfVectorizer(ngram_range=(1,3), max_features=60000, stop_words=\"english\")\n    vectorizer.fit(df_train[\"text\"])\n\n    X_train = vectorizer.transform(df_train[\"text\"])\n    y_train = df_train[\"label\"].values\n\n    X_val = vectorizer.transform(df_val[\"text\"])\n    y_val = df_val[\"label\"].values\n\n    X_test = vectorizer.transform(df_test[\"text\"])\n    y_test = df_test[\"label\"].values\n\n    # Base models (Version B2 params)\n    models = get_base_models_B2()\n    val_preds, test_preds = [], []\n\n    for name, model in models.items():\n        print(f\"[{test_year}] Training base model: {name}\")\n        model.fit(X_train, y_train)\n        val_probs = model.predict_proba(X_val)[:, 1]\n        test_probs = model.predict_proba(X_test)[:, 1]\n        val_preds.append(val_probs)\n        test_preds.append(test_probs)\n\n    # Meta learner (stacking)\n    X_val_stack = np.vstack(val_preds).T\n    X_test_stack = np.vstack(test_preds).T\n    meta = LogisticRegression(solver=\"liblinear\", class_weight=\"balanced\", random_state=RANDOM_STATE)\n    meta.fit(X_val_stack, y_val)\n\n    val_scores = meta.predict_proba(X_val_stack)[:, 1]\n    test_scores = meta.predict_proba(X_test_stack)[:, 1]\n\n    # Threshold tuning (validation only)\n    best_thr, best_f2 = pick_threshold(y_val, val_scores, beta=2.0)\n    print(f\"[{test_year}] Meta OOF best threshold={best_thr:.4f}, OOF-F2={best_f2:.3f}\")\n\n    # Apply to test\n    y_pred_test = (test_scores >= best_thr).astype(int)\n\n    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_test, labels=[0, 1]).ravel()\n    f1 = f1_score(y_test, y_pred_test)\n\n    print(f\"\\n=== Year {test_year} Results ===\")\n    print(f\"TP={tp}, FP={fp}, FN={fn}, F1={f1:.3f}\")\n    print(f\"[{test_year}] Stored {len(df_test)} test rows, predicted 1s: {int(np.sum(y_pred_test))}\\n\")\n\n    df_test_result = df_test[['year', 'text', 'firm_id']].copy()\n    df_test_result['true_label'] = y_test\n    df_test_result['pred_label'] = y_pred_test\n    df_test_result['pred_score'] = test_scores\n    df_test_result['used_threshold'] = best_thr\n\n    return df_test_result\n\n# ---------------------------\n# Runner across years\n# ---------------------------\ndef run_all_years_version_b2(df, start_year=2000, end_year=2021):\n    all_results = []\n    df_all_predicted = []\n    cumulative = {'TP': 0, 'FP': 0, 'FN': 0}\n\n    for year in range(start_year, end_year + 1):\n        print(f\"\\n########### START YEAR {year} ###########\")\n        res = evaluate_year_version_b2(df, year, beta=2)\n        if res is None:\n            print(f\"[{year}] skipped.\")\n            continue\n\n        df_all_predicted.append(res)\n        tn, fp, fn, tp = confusion_matrix(res['true_label'], res['pred_label'], labels=[0, 1]).ravel()\n        f1 = f1_score(res['true_label'], res['pred_label'])\n        all_results.append({'year': year, 'TP': tp, 'FP': fp, 'FN': fn, 'F1': f1})\n\n        cumulative['TP'] += tp\n        cumulative['FP'] += fp\n        cumulative['FN'] += fn\n        cumF1 = (2 * cumulative['TP']) / (2 * cumulative['TP'] + cumulative['FP'] + cumulative['FN']) if (2 * cumulative['TP'] + cumulative['FP'] + cumulative['FN']) > 0 else 0.0\n        print(f\"=== End YEAR {year} === Cumulative F1 ({start_year}-{year}) = {cumF1:.3f}\")\n        print(f\"########### END YEAR {year} ###########\\n\")\n\n    df_all_predicted = pd.concat(df_all_predicted, ignore_index=True) if df_all_predicted else pd.DataFrame(columns=['year','text','firm_id','true_label','pred_label','pred_score','used_threshold'])\n    df_yearly = pd.DataFrame(all_results).sort_values('year').reset_index(drop=True)\n\n    # Overall\n    overall_TP = df_yearly['TP'].sum() if not df_yearly.empty else 0\n    overall_FP = df_yearly['FP'].sum() if not df_yearly.empty else 0\n    overall_FN = df_yearly['FN'].sum() if not df_yearly.empty else 0\n    overall_F1 = (2 * overall_TP) / (2 * overall_TP + overall_FP + overall_FN) if (2 * overall_TP + overall_FP + overall_FN) > 0 else 0.0\n\n    print(\"\\n=== Overall Summary across all years ===\")\n    print(f\"Overall TP={overall_TP}, FP={overall_FP}, FN={overall_FN}, F1={overall_F1:.3f}\")\n\n    return df_all_predicted, df_yearly\n\n# ---------------------------\n# Example usage (execution)\n# ---------------------------\nif __name__ == \"__main__\":\n    # Expect df_all present in environment with columns: year, firm_id, text, label, source\n    try:\n        df_all\n    except NameError:\n        raise RuntimeError(\"Load df_all first (pandas DataFrame) with columns: year, firm_id, text, label, source\")\n\n    # Run pipeline\n    preds_df, yearly_metrics = run_all_years_version_b2(df_all, start_year=2000, end_year=2021)\n\n    # Extract predicted 1s and save\n    df_predicted_1s = preds_df[preds_df['pred_label'] == 1][['year','firm_id','text','true_label','pred_score','used_threshold']]\n    print(f\"\\nTotal predicted 1s across all years: {len(df_predicted_1s)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:09:18.423358Z","iopub.execute_input":"2025-09-28T06:09:18.423694Z","iopub.status.idle":"2025-09-28T06:20:07.167989Z","shell.execute_reply.started":"2025-09-28T06:09:18.423662Z","shell.execute_reply":"2025-09-28T06:20:07.166900Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pickle\n\n# --- Save ---\nwith open(\"df_predicted_1s_4th_best.pkl\", \"wb\") as f:\n    pickle.dump(df_predicted_1s, f)\n\nprint(\"✅ df_predicted_1s_4th_best saved to df_predicted_1s_4th_best.pkl\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:20:07.172531Z","iopub.execute_input":"2025-09-28T06:20:07.172783Z","iopub.status.idle":"2025-09-28T06:20:07.196605Z","shell.execute_reply.started":"2025-09-28T06:20:07.172756Z","shell.execute_reply":"2025-09-28T06:20:07.195877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Save as CSV\ndf_predicted_1s.to_csv(\"df_predicted_1s_4th_best.csv\", index=False, encoding='utf-8')\nprint(\"✅ df_predicted_1s_4th_best saved to df_predicted_1s_4th_best.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-28T06:20:07.197456Z","iopub.execute_input":"2025-09-28T06:20:07.197856Z","iopub.status.idle":"2025-09-28T06:20:07.493085Z","shell.execute_reply.started":"2025-09-28T06:20:07.197809Z","shell.execute_reply":"2025-09-28T06:20:07.492169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}